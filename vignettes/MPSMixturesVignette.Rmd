---
title: "Deconvolution of MPS STR DNA mixtures and approximation of the likelihood ratio"
author: "SÃ¸ren B. Vilsen"
date: "`r Sys.Date()`"
bibliography: Literature.bib
output: 
    pdf_document:
        includes:
            in_header: preamble.tex
        keep_tex: yes
        number_sections: true
vignette: >
  %\VignetteIndexEntry{Deconvolution of MPS STR DNA mixtures and approximation of the likelihood ratio}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

# Introduction
The central question in forensic genetics is determination of the posterior odds of two competing hypotheses. Assume DNA evidence, $\mc E$, has been collected at a crime-scene. Furthermore, if we let $\mc H_p$ and $\mc H_d$ denote the hypothesis of the prosecutions and defences, respectively, then the posterior odds are given by:
\begin{align}
\frac{\p{\mc H_d|\mc E}}{\p{\mc H_p|\mc E}} = \frac{\p{\mc E|\mc H_d}}{\p{\mc E|\mc H_p}}\frac{\p{\mc H_d}}{\p{\mc H_p}}.
\end{align}

The prior probabilities of the two hypotheses are either assumed equal or supplied by the court, which leaves us with the likelihood ratio:
\begin{align}
\text{LR}(\mc H_d, \mc H_p) = \frac{\p{\mc E|\mc H_d}}{\p{\mc E|\mc H_p}}.
\end{align}

Assuming that the evidence $\mc E$ consists of some quantitative information $\bs{\mc Q}$ (the coverage in MPS or peak height/area in CE) about some genetic information $\bs \mcg$, we can factorise $\p{\mc E|\mc H_i}$ as: 
\begin{align}
\p{\mc E|\mc H_i} = \p{\bs{\mc Q}, \bs \mcg|\mc H_i} = \p{\bs{\mc Q}|\bs \mcg}\p{\bs \mcg|\mc H_i}.
\end{align}

Thus, the probability of the evidence, given a hypothesis, has two parts: (1) The probability of the quantitative information given allelic information, and (2) The probability of the allelic information under the hypothesis in question. The population is assumed either in Hardy-Weinberg equilibrium (HWE) or it is assumed to be inbred, which is accounted for using $\theta$ correction [@Balding1994].

If a hypothesis states that a contributor is unknown, it becomes necessary to sum over the set of possible unknown contributors to the mixture, in order to correctly state the probability of the evidence. That is, the probability of the evidence in the case of unknown contributors is:
\begin{align*}
\p{\mc E|\mc H_i} &= \sum_{\bs \mcg_{u} \in \mc U_i} \p{\bs{\mc Q}|\bs \mcg_{K_i}, \bs \mcg_{u}}\p{\bs \mcg_{u}|\bs \mcg_{K}}
\end{align*}
where $\bs \mcg_{K_i}$ are the known contributors under hypothesis $i$, $\bs \mcg_{K} = \{\bs \mcg_{K_d}, \bs \mcg_{K_p}\}$, and $\mc U_i$ is the set of possible unknown contributors under hypothesis $i$.

The sum over the set of unknown contributors may be intractable. Therefore, we search through the space of unknown contributors using an Evolutionary Algorithm. The algorithm will have two objectives:
\begin{enumerate}[(1)]
\item Find the combination of unknown genotypes maximising the $\p{\bs{\mc Q}|\bs \mcg_{K_i}, \bs \mcg_{u}}\p{\bs \mcg_{u}|\bs \mcg_{K}}$, under an hypoethesis $\mc H_i$.
\item Approximate the likelihood ratio by approximating the set of combinations of unknown contributors by the elements having the largest contribution to the sum.
\end{enumerate}

# The coverage model
We define the coverage model in two parts: (1) the allele coverage model, which will account for the alleles of the contributors and any systematic errors produced by the sequencing process, and (2) a noise coverage model accounting for the remaining observations.

The allele coverage model, introduced in [@Vilsen2017b], was adaptated from capillary electrophoresis peak height/width models [@tvedebrink_etal_2013], [@taylor_etal_2013], [@cowell_etal_2015], [@oyvind_bleka_2016], and [@steele_2016], to model the allele coverage generated by the MPS process. The coverage of a string is the number of times the string is observed (i.e. the count). Therefore, we model the coverage as overdispersed count data. That is, the coverage of allele $a$ marker $m$ is distributed as:
\begin{align*}
y_{ma} \sim \text{PG1}\left(\mu_{ma}, \gamma\right),
\end{align*}
where PG1 is the Poisson-gamma distribution (the mean parametrised negative binomial distribution) of order 1 (the variance is $\mu_{ma} (1 + \gamma)$), $\mu_{ma}$ the expected coverage, and $\gamma$ the overdispersion. Furthermore, we assume that the expected coverage is given as:
\begin{align*}
\mu_{ma} = \nu \beta_m \sum_{c} \left[g_{mac} + \sum_{A\,\in\;\mc P(a)} \xi_{mA} g_{mAc} \right]\varphi_c,
\end{align*}
where $\nu$ is the average heterozygote coverage, $\beta_m$ is a marker dependent scaling parameter, with $\sum \beta_m = M$, $\varphi_c$ is the relative contribution of contributor $c$, with $\sum_c \varphi_c = 1$, $g_{mac}$ is the genotype of marker $m$ allele $a$ contributor $c$, $\mc P(a)$ is the set of potential parents of allele $a$ and $\xi_{mA}$ is the stutter ratio of parent $A$. Note that the parameters $\bs\beta$ and $\bs\xi$, as well as the set of potential parents are assumed known throughout this vignette.  

Any observation not identified as an allele or a stutter of an allele is classified as noise and denoted using set complement notation: $\bs y^c$ and $\bs g^c$. The coverage of the noise is assumed to follow a zero truncated Poisson-gamma distribution with mean $\psi$ and overdispersion $\rho$. 

Assuming that the two parts are independent given the genotypic information, then the likelihood can be written as:
\begin{align}\label{eq:likelihoodModel}
\p{\bs{\mc Q}|\bs \mcg_{K_i}, \bs \mcg_{u}} = \prod_m L(\nu, \eta, \bs \varphi | \beta_m, \bs\xi_m, \bs y_m, \bs g_m) L(\psi, \rho | \bs y^c_m, \bs g^c_m)
\end{align}

# Example
```{r hidden, echo = FALSE}
load_rdata <- function(fileName) {
    load(fileName)
    get(ls()[ls() != "fileName"])
}
```

We will need the following `R` packages: 
```{r loadPackages, message = FALSE}
library("Biostrings")
library("tidyverse")
library("stringr")
library("Rsolnp")
library("MPSMixtures")
library("microbenchmark")
```

First, we need to make some assumption about the popoulation from which these alleles are drawn. That is, we need an allelic ladder of this population and we need to make assumptions on the allele frequencies and the inbreding coefficient. The `MPSMixtures` package has an example ladder and corresponding frequencies called `examplePopulation`. For simplicity, we will assume that any allele combination is equally likely. That is, eliminating the genotypic distribution entirely. This is done to make the deconvolution results easily comparable with the truth. In the package, the genotypic distribution is ignored, by assining a negative value to the inbreeding coefficient ($\theta$):
```{r populationParameters}
theta = -1.0
data(examplePopulation)
```

An allelic ladder can also be generated using the `generateAllelicLadder` function, specifying the possible regions and their frequncies, using the arguments `markers`, `regions`, `frequencies`, and if necessary `motifLength`. Note: all three vectors need to be of equal length (or length 1). The code used to generate the `examplePopulation` can be seen in the example of the `generateAllelicLadder` function.

As we cannot use a real DNA profile as an example, we will simulate data given a fictional case of murder. To simulate the coverage, in this case of murder and mayham, we will assume the following: the marker imbalances, a stutter ratio model, the number of observed alleles for each marker, the true profiles of the contributors (which we will simulate), and the parameters of the model. 

The number of markers is $M = 10$ and we sample the marker imbalances, $\bs\beta$, using a gamma distribution:
```{r markerImbalances}
set.seed(123456)
markers = unique(examplePopulation$Marker)
numberOfMarkers = length(markers)

markerImbalances <- rgamma(numberOfMarkers, shape = 10, rate = 10)
markerImbalances <- markerImbalances / mean(markerImbalances)
markerImbalances
```

We will assume that stutter ratio is consistent across markers (which is very unrealistic), and create the stutter ratio model, as: 
```{r}
stutterRatioTibble <- 
    tibble(BlockLengthMissingMotif = sample(1:40, 1000, replace = T),
           StutterRatio = 0.005 * BlockLengthMissingMotif + rnorm(1000, 0, 0.02)) %>% 
    mutate(StutterRatio = ifelse(StutterRatio < 0, 0, StutterRatio))

stutterRatioModel = lm(StutterRatio ~ 0 + I(BlockLengthMissingMotif - 1), 
                       data = stutterRatioTibble)
```

Given the number of contributors, and the population data generated above, we sample the genotypes from the population assuming HWE.
```{r genotypes} 
numberOfContributors = 2
trueProfiles <- sampleGenotypesHWE(numberOfContributors, examplePopulation, c("V", "P"))
```

The parameters in the allele coverage model are:
```{r alleleParameters}
nu = 1400
gamma = 8
phi = c(0.7, 0.3)
```

The parameters in the noise coverage model are: 
```{r noiseParameters}
psi = 3
rho = 2
```

The coverage can now be simulated using the `sampleCoverage` function, returning a `tibble` containing the sampled coverage:
```{r sampledCoverage}
sampleTibble = sampleCoverage(
    trueProfiles = trueProfiles, markerImbalances = markerImbalances,
    populationLadder = examplePopulation, stutterRatioModel = stutterRatioModel,
    alleleCoverageParameters = list(nu = nu, eta = gamma, phi = phi),
    noiseParameters = list(psi = psi, rho = rho, maxElements = 20), 
    p = NULL)
```

Any user defined data-set must have the same structure and column names as the `sampleTibble` `tibble`. The `sampleTibble`-tibble is combined with `examplePopulation` in such a way that the regions in `examplePopulation` not seen in `sampleTibble` are added with a coverage of 0. This allows us to account for drop-outs in the model, while drawing on the allele frequencies for information. Note that any region atributed to the noise distribution with a coverage of zero is ignored.

We have written this package with two objectives in mind: 
\begin{enumerate}[(1)]
\item Find the combination of unknown genotypes maximising the $L\left(\bs{y}|\bs \mcg_{K_i}, \bs \mcg_{u}\right)\p{\bs \mcg_{u}|\bs \mcg_{K}}$, under some hypothesis $\mc H_i$.
\item Approximate the likelihood ratio by approximating the set of combinations of unknown contributors, by using the elements having the largest contribution to the sum, see Eq.\eqref{eq:appLR}.
\end{enumerate}

Furthermore, we will build and store the list of potential parents. As the data does not change, we do not want to rebuild it every single time we initialise an algorithm:
```{r potentialParentsList}
potentialParentsList <- potentialParentsMultiCore(sampleTibble, stutterRatioModel)
```

Note: we can also calculate the fitness and estimate the parameters, assuming that both profiles are known, by calling the `estimateParametersOfKnownProfiles` function. 
```{r bothKnown}
knownProfilesIndividual <- 
    estimateParametersOfKnownProfiles(sampleTibble, markerImbalances, trueProfiles, 
                                      potentialParentsList, 
                                      NULL, 1, 0.8, rep(1e-8, 4))

knownProfilesIndividual[c("Parameters", "LogLikelihoods", "Fitness")]
```

The `MPSMixtures` offers two simple fit evaluation plots: Q-Q plots of the deviance residuals and prediction intervals for the expected coverage. They are created by calling the `ggplotQQPlotProfiles` and `ggplotPredictionIntervals` functions, respectively. The resulting figures can be seen in Figure \ref{fig:qqtrue} and \ref{fig:ectrue}.

```{r figQQcache, echo=FALSE, cache=TRUE}
figQQcache <- ggplotQQPlotProfilesEnvelopes(sampleTibble = sampleTibble, profileList = knownProfilesIndividual, 
                                            markerImbalances = knownProfilesIndividual$Parameters$MarkerImbalanceParameters,
                                            knownProfilesList = trueProfiles, potentialParentsList = potentialParentsList)

figPPcache <- ggplotPPPlotProfilesEnvelopes(sampleTibble = sampleTibble, profileList = knownProfilesIndividual, 
                                            markerImbalances = knownProfilesIndividual$Parameters$MarkerImbalanceParameters,
                                            knownProfilesList = trueProfiles, potentialParentsList = potentialParentsList)
```


```{r figQQ, echo = FALSE, fig.align = "center", fig.cap = "\\label{fig:qqtrue}Q-Q plot of the fitted model for both the allele and noise coverage models.", fig.width = 4, fig.height = 4, fig.pos="ht!", out.width='.49\\linewidth', fig.show='hold'}
show(figQQcache)
show(figPPcache)
```

```{r figEC, echo = FALSE, fig.align = "center", fig.cap = "\\label{fig:ectrue}Bar-plot of the coverage against the allele length for the each marker and profile, shown in coloumns and rows, respectively. Furthermore, the prediction interval for each observation is seen as the black bars, with the expected coverage at their center.", fig.width = 17, fig.height = 7, fig.pos="ht!"}
ggplotPredictionIntervals(sampleTibble, knownProfilesIndividual, contributorNames = c("V", "P"))
```

## Finding the best combination of unknown genotypes {#sec:maximalUnknown}
As two is the total number of contributors, we have three possible spaces of unknown genotypes. The spaces, when (1) the minor (perpetrator) is known, (2) the major (victim) is known, and (3) both contributors are unknown. 

We will take each case in turn, trying to ascertain the optimal genotype (or combination of genotypes), under the model. Furthermore, we will, only in the first case, use both a single population and multiple population model. In cases two and three, we will stick to the multiple population approach, as it is more than ten times faster. 

Note that if the genotypic information (allele frequencies) in the population should be ignored, we can set $\theta < 0$ or set all allele frequencies to zero. 

### Known minor
We create an hypothesis using the `setHypothesis` function. The known profiles should passed as a list containing an element for each of the known profiles (in this case the perpetrator).
```{r perpProfile}
knownPerpetrator <- setHypothesis(sampleTibble, numberOfContributors, trueProfiles["P"], theta)
```

We start with the single population algorithm:
```{r singleMajor, eval = FALSE}
controlSinglePopulation <-  
    optimalUnknownProfileCombination.control(
        numberOfPopulations = 1, numberOfIterations = 350,
        populationSize = 100, numberOfFittestIndividuals = 1,
        mutationDecayRate = 1, hillClimbingIterations = 0, 
        parentSelectionWindowSize = 15,
        allowParentSurvival = TRUE, trace = F, #trace = T, traceLimit = 5, 
        levelsOfStutterRecursion = 1, 
        numberOfIterationsEqualMinMax = 10, 
        tolerance = rep(1e-8, 4)
    )

singlePopulationBenchmarkMajor <- microbenchmark(
    optimalSingleMajor <- 
        optimalUnknownProfileCombination(sampleTibble = sampleTibble, 
                                         markerImbalances = markerImbalances, 
                                         H = knownPerpetrator[[1]], 
                                         potentialParentsList = potentialParentsList, 
                                         control = controlSinglePopulation),
    times = 1)
```

```{r singleMajor2, echo = FALSE}
controlSinglePopulation <-  
    optimalUnknownProfileCombination.control(
        numberOfPopulations = 1, numberOfIterations = 350,
        populationSize = 2000, numberOfFittestIndividuals = 1,
        mutationDecayRate = 1, hillClimbingIterations = 0, 
        parentSelectionWindowSize = 15,
        allowParentSurvival = TRUE, trace = F, 
        levelsOfStutterRecursion = 1, 
        numberOfIterationsEqualMinMax = 10, 
        tolerance = rep(1e-8, 4)
    )

optimalSingleMajor <- load_rdata(system.file('extdata', "singleMajor.RData", package = 'MPSMixtures'))
singlePopulationBenchmarkMajor <- load_rdata(system.file('extdata', "singleMajorBench.RData", package = 'MPSMixtures'))
```


The single population algorithm terminated in `r round(singlePopulationBenchmarkMajor$time / 1e9, 2)` seconds.

The fittest found individual is always listed as the first entry in the returned list. In this case, the log-likelihood in the fittest found individual is `r round(optimalSingleMajor$U[[1]]$Fitness, 4)`, with the log-likelihoods of the allele, noise, and genotype probabilities at `r round(optimalSingleMajor$U[[1]]$LogLikelihoods[1], 4)`, `r round(optimalSingleMajor$U[[1]]$LogLikelihoods[2], 4)`, and `r round(optimalSingleMajor$U[[1]]$LogLikelihoods[3], 4)`. That is, the found major contributor is the same as the true major contributor and the increase in fitness is solely due to the added uncertainty represented by the genotype probability. The corresponding estimated parameters are (the same as seen above):

```{r sampleParmsSingleMajor}
optimalSingleMajor$U[[1]][["Parameters"]]
```

The relative difference for the parameters $\nu$, $\gamma$, $\psi$, $\rho$, and $\bs \varphi$ are given as: `r round(abs(nu - optimalSingleMajor$U[[1]]$Parameters[["SampleParameters"]][1]) / nu, 2)`, `r round(abs(gamma - optimalSingleMajor$U[[1]]$Parameters[["SampleParameters"]][2]) / gamma, 2)`, `r round(abs(psi - optimalSingleMajor$U[[1]]$Parameters[["NoiseParameters"]][1]) / psi, 2)`, `r round(abs(rho - optimalSingleMajor$U[[1]]$Parameters[["NoiseParameters"]][2]) / rho, 2)`, and `r round(abs(phi[2] - optimalSingleMajor$U[[1]]$Parameters[["MixtureParameters"]][1]) / phi[2], 2)`, respectively. We see that the $\hat \nu$, $\hat \gamma$, $\hat \psi$, and $\hat{\bs \varphi}$ are all within 10\% of the true values, only the overdispersion parameter of the noise distribution, $\hat \rho$, has proven difficult to estiamte. 

We will use 32 sub-populations spread on four cores. Furthermore, we set the size of each population to 75, i.e. a total population size of 2,400.

```{r multipleMajor, eval = FALSE}
controlMultiplePopulation <- 
    optimalUnknownProfileCombination.control(
        numberOfPopulations = 16, numberOfIterations = 100,
        populationSize = 25, numberOfFittestIndividuals = 1,
        numberOfIterationsEqualMinMax = 10,
        mutationDecayRate = 1, hillClimbingIterations = 0, 
        parentSelectionWindowSize = 6,
        allowParentSurvival = TRUE, trace = T, traceLimit = 5, #trace = F, 
        levelsOfStutterRecursion = 1, 
        tolerance = rep(1e-8, 4)
    )

multiplePopulationBenchmarkMajor <- microbenchmark(
    optimalMultipleMajor <- 
        optimalUnknownProfileCombination(sampleTibble = sampleTibble, 
                                         markerImbalances = markerImbalances, 
                                         H = knownPerpetrator[[1]], 
                                         potentialParentsList = potentialParentsList, 
                                         control = controlMultiplePopulation),
    times = 1)

save(optimalMultipleMajor, file = "../inst/extdata/multipleMajor.RData")
save(multiplePopulationBenchmarkMajor, file = "../inst/extdata/multipleMajorBench.RData")
```

```{r multipleMajor2, echo = FALSE}
controlMultiplePopulation <- 
    optimalUnknownProfileCombination.control(
        numberOfPopulations = 32, numberOfIterations = 100,
        populationSize = 75, numberOfFittestIndividuals = 1,
        numberOfIterationsEqualMinMax = 10,
        mutationDecayRate = 1, hillClimbingIterations = 0, 
        parentSelectionWindowSize = 6,
        allowParentSurvival = TRUE, trace = F, 
        levelsOfStutterRecursion = 1, 
        tolerance = rep(1e-8, 4)
    )

optimalMultipleMajor <- load_rdata(system.file('extdata', "multipleMajor.RData", package = 'MPSMixtures'))
multiplePopulationBenchmarkMajor <- load_rdata(system.file('extdata', "multipleMajorBench.RData", package = 'MPSMixtures'))
```

The parallel EA terminated in `r round(multiplePopulationBenchmarkMajor$time / 1e9, 2)` seconds, i.e. close to ten times faster than the single population algorithm. Thus, we see both an increased performance from the parallelisation and the reduction in population size. Furthermore, the fittest found individual had a fitness of `r round(optimalMultipleMajor$U[[1]]$Fitness, 4)`. That is, the two approaches have terminated with the same individual as the fittest individual.

In both cases the log-likehoods and fitness of the fittest individual are given as:
```{r multipleMajorParameters}
optimalMultipleMajor$U[[1]][c("LogLikelihoods", "Fitness", "Parameters")]
```

As the parallel EA was both faster and got the exact same result, we will, in the remainder of this manuscript, only use the parallel EA. 

### Known major
Now let us assume that the major is known (i.e. the victim):
```{r}
knownVictim <- setHypothesis(sampleTibble, numberOfContributors, trueProfiles["V"], theta)
```

We will use the same control settings, `controlMultiplePopulation`, as in the previous section. 
```{r multipleMinor, eval = FALSE}
optimalMultipleMinor <- optimalUnknownProfileCombination(sampleTibble = sampleTibble,
                                                         markerImbalances = markerImbalances, 
                                                         H = knownVictim[[1]], 
                                                         potentialParentsList = potentialParentsList, 
                                                         control = controlMultiplePopulation)
```

```{r multipleMinor2, echo = FALSE}
optimalMultipleMinor <- load_rdata(system.file('extdata', "multipleMinor.RData", package = 'MPSMixtures'))
```


The parameters, log-likelihoods, and fitness are:
```{r multipleMinorParameters}
optimalMultipleMinor$U[[1]][c("Parameters", "LogLikelihoods", "Fitness")]
```

Comparing the log-likelihoods of the optimal minor contributor to those of the true profile, we see that the log-likelihood of the noise model does not change, while the log-likelihood of the allele coverage is smaller for the optimal minor than for the true minor. However, we see that the decrease in the log-likehood allele coverage is outweighed by an increase in the genotype probability.

### Both profiles unknown
Lastly, we assumed that both the profiles were unknown. This is indicated by supplying an empty list (or just `NULL`) to the knownProfiles argument. Furthermore, we have increased the number of sub-populations, the population size, allowed for more outer itertations, and increased the termination counter `numberOfIterationsEqualMinMax`, giving the algorithm more times to search the space. 
```{r multipleUnknown, eval = FALSE}
bothUnknown <- setHypothesis(sampleTibble, numberOfContributors, list(), theta)

controlMultiplePopulation <- 
    optimalUnknownProfileCombination.control(
        numberOfPopulations = 64, 
        numberOfIterations = 100,
        populationSize = 300,
        numberOfFittestIndividuals = 1,
        numberOfIterationsEqualMinMax = 50,
        hillClimbingIterations = 0,
        mutationDecayRate = 1,
        parentSelectionWindowSize = 12,
        trace = FALSE, 
        tolerance = rep(1e-8, 4)
    )

multiplePopulationBenchmarkUnknown <- microbenchmark(
    optimalMultipleUnknown <- 
        optimalUnknownProfileCombination(sampleTibble = sampleTibble,
                                         markerImbalances = markerImbalances,
                                         H = bothUnknown[[1]],
                                         potentialParentsList = potentialParentsList,
                                         control = controlMultiplePopulation),
    times = 1)
```

```{r , echo = FALSE}
bothUnknown <- setHypothesis(sampleTibble, numberOfContributors, list(), theta)

controlMultiplePopulation <- 
    optimalUnknownProfileCombination.control(
        numberOfPopulations = 64, 
        numberOfIterations = 100,
        populationSize = 300,
        numberOfFittestIndividuals = 1,
        numberOfIterationsEqualMinMax = 50,
        hillClimbingIterations = 0,
        mutationDecayRate = 1,
        parentSelectionWindowSize = 12,
        trace = FALSE, 
        tolerance = rep(1e-8, 4)
    )

optimalMultipleUnknown <- load_rdata(system.file('extdata', "multipleUnknown.RData", package = 'MPSMixtures'))
multiplePopulationBenchmarkUnknown <- load_rdata(system.file('extdata', "multipleUnknownBench.RData", package = 'MPSMixtures'))
```

The algorithm terminated in 
seconds. The parameters, log-likelihoods, and fitness of the optimal unknown genotype combination is:

```{r multipleUnknownParameteres}
optimalMultipleUnknown$U[[1]][c("Parameters", "LogLikelihoods", "Fitness")]
```

We see that the optimal combination of unknown genotypes is equal to the true major and optimal minor, i.e. the differences can be seen in Figure \ref{fig:minor}. This, there was not a huge difference between the true and the optimal genotype combinations, and as above the difference was largely down to the allele frequencies of the population. 

## Approximating the LR
In our case of ficticious murder, we will assume that the victim is known and its profile typed. We will define the following competing hypotheses:
\begin{itemize}
\item The prosecution hypothesis, $\mc H_p$: The suspect, with genotype $\bs g_S$, is the perpetrator. 
\item The defence hypothesis, $\mc H_d$: Another random person from the population is the perpetrator.
\end{itemize}

In `R` we define a hypothesis by the `setHypothesis` function. It takes the arguments: `sampleTibble`, the total number of contributors to the mixture, `numberOfContributors`, a list of the known profiles to the mixture, `knownProfilesList`, and the inbreeding coefficient, `theta`.
```{r hypotheses}
theta = 0
knownProfilesHp <- trueProfiles
knownProfilesHd <- trueProfiles["V"]

Hp <- setHypothesis(sampleTibble, numberOfContributors, knownProfilesHp, theta)
Hd <- setHypothesis(sampleTibble, numberOfContributors, knownProfilesHd, theta)
```

An object of class `hypothesis` holds the total number of contributors, the number of known contributors, the combined genotype matrix of the known contributors, the inbreeding coefficient, and the allele frequencies of the chosen population. 

Given the `sampleTibble` and the two competing hypotheses, we can initialise the EA for obtaining an approximate likelihood ratio by calling the `LR` function. As before we run the parallel algorithm with twelve sub-populations.

```{r multipleLRMinor, eval = FALSE}
LRParallelPopulations <-
    LR(sampleTibble = sampleTibble, Hp = Hp, Hd = Hd, markerImbalances = markerImbalances,
       potentialParentsList = potentialParentsList, stutterRatioModel = NULL,
       control = optimalUnknownProfileCombination.control(
           numberOfPopulations = 32, numberOfIterations = 150,
           populationSize = 75, numberOfFittestIndividuals = 1000,
           hillClimbingIterations = 0, mutationDecayRate = 1,
           parentSelectionWindowSize = 6, simplifiedReturn = FALSE,
           allowParentSurvival = TRUE, trace = FALSE, 
           tolerance = rep(1e-8, 4)))
```

```{r multipleLRMinor2}
LRParallelPopulations <- load_rdata(system.file('extdata', "multipleLRMinor.RData", package = 'MPSMixtures'))
```

The `hypothesis` class can take more than a single prosecutors or defence hypothesis (and, therefore, so can the `LR` function), and the `LR` function returns the likelihood ratios of every pairwise comparison of these hypotheses. The comparison table is accessed in the returned list as:

```{r multipleLRMinorTable}
LRParallelPopulations$ComparisonTable
```

# References
